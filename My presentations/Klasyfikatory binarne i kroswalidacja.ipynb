{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier\n",
    "It takes only two values: \"0\" or \"1\", \"true\" or \"false\" etc.\n",
    "Examples:\n",
    "- Covid Test (checks, is someone's sick or not),\n",
    "- Analysis, if someone will be able to pay back credit,\n",
    "- finding out if we talk with real person or AI.\n",
    "\n",
    "## Values Analysis with COVID example\n",
    "\n",
    "|  | someone is sick | someone is healthy |\n",
    "| --- | --- | --- |\n",
    "| **result of PCR test is positive** | true positive (TP) | false positive (FP) |\n",
    "| **result of PCR test is negative** | false negative (FN) | true negative (TN) |\n",
    "\n",
    "False positive are called **type I errors**, while false negative are called **type II errors**. Obviously we want to maximize rate of true negatives and positives while minimizing rate of the errors - so minimize FP/TP and FN/TN. However sometimes its not so simple.\n",
    "\n",
    "What's wrong with a classifier that simply maximizes rate TP and TN and meanwhile minimizes rate of FN and FP? Imagine we have a test that detects some rare illness X. If someone is sick, tests always shows (true) positive result. If someone is not sick, test will in 99% cases show (true) negative result and in 1% (false) positive result. Seems pretty accurate? Nope. If only every 1 per 10 000 people is sick of X, then, testing 10 000 people, we will have 101 people diagnosed as ill, while only 1 person is really sick. Conclusion: we need better tools.\n",
    "\n",
    "# Our tools and its values on COVID tests.\n",
    "Here is some research testing quality of rapid antigene tests in comparison to PCR-tests. \n",
    "\n",
    "https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2793365?utm_source=For_The_Media&utm_medium=referral&utm_campaign=ftm_links&utm_term=061522\n",
    "\n",
    "Results: \n",
    "| | |\n",
    "|---|---|\n",
    "|TP = 50| FP = 1|\n",
    "|FN = 27| TN = 649|\n",
    "\n",
    "- **sensitivity** (true positives rate) - probability of correctly diagnosing sick person:\n",
    "$$TPR = \\frac{TP}{TP+FN} = 65\\%$$\n",
    "- **specificity** (true negative rate) - probability of correctly diagnosing healthy person: \n",
    "$$SPC = \\frac{TN}{FP+TN} = 100\\%$$\n",
    "- **false positive rate** - probability of wrongly diagnosing a healthy person as a sick one:\n",
    "$$FPR = \\frac{FP}{FP+TN} = 0\\%$$\n",
    "- **false discovery rate** - if someone is diagnosed as sick, what is the chance that he is actually healthy:\n",
    "$$FDR = \\frac{FP}{FP+TP} = 2\\%$$\n",
    "- **positive predicted value** - if someone is diagnosed as sick, what is the chance that he is really sick:\n",
    "$$PPV = \\frac{TP}{FP+TP} = 98\\%$$\n",
    "- **negative predicted value** - if someone is diagnosed as healthy, what is the chance that he is really healthy:\n",
    "$$NPV\\frac{TN}{TN+FN} = 96\\%$$\n",
    "- **accuracy** - probability of a correct diagnose:\n",
    "$$ACC = \\frac{TP+TN}{TP+TN+FP+FN} = 96\\%$$\n",
    "\n",
    "# ROC curve\n",
    "\n",
    "In the case of binary classifiers we often have a continuous input and 2-valued output. To achieve such result we may add a threshold - if input is under threshold, then output = 0 and if it is above it then output = 1. Example:\n",
    "\n",
    "<img src=\"ROC_rozklady.png\" alt=\"Threshold\" width=\"800\"/>\n",
    "\n",
    "ROC curve is a way of visualizing a threshold. We create a plot with TPR on Y-axis and FPR on X-axis. Each choice of threshold results in a single point in the plot. Its not only nice way to visualize data, but also may be helpful in choosing right classifier.\n",
    "\n",
    "<img src=\"Roc_curve.svg\" alt=\"ROC\" width=\"400\"/>\n",
    "\n",
    "For a fast judgement if a classifier is good, one can compute the area under the ROC curve (we call it **AUC**). For a perfect classifier it's equal to 1, for a random one is qual to 0.5.\n",
    "\n",
    "# Cross Validation\n",
    "\n",
    "How to judge which of two models is better? Simplest case: divide data into **train data** and **test data**. Then we train both model on training part and compare their errors ($RSS$ or $R^2$) on testing part. Drawback: we \"lose\" a big part of data - test data is not used to train model. That's when cross validation comes in handy.\n",
    "\n",
    "**How cross validation works?**\n",
    "1. We divide data in $k$ parts (we'll call them folds). Usually $k = 5, 10$.\n",
    "2. We train model $k$ times. Each time we choose different fold to be our \"test data\", while we train model on the other $k-1$ - folds.\n",
    "3. For each trained model we compute test error (RSS). The mean of those errors is called **validation error**.\n",
    "4. The better model, the smaller is validation error. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src = \"cross_validation.png\" alt = CV width = \"800\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
