{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is learning rate?\n",
    "In neural networks we want to find parameters (weights) that will minimize loss function. To achieve that, firstly we consider a weights space. In case of linear regression ($y=ax+b$) it's simply $\\mathbb{R}^2\\ni (a,b)$. We choose first point at random and with each step we are moving in direction of greatest decrease of the loss function - so in opposite direction of the gradient of loss function with respect to weights.\n",
    "\n",
    "How fast should we going? If we go too fast, we won't be able to stop in the minimum. If we go too slow, it's gonna take ages.\n",
    "\n",
    "<img src = \"LR.png\" alt = \"LR\" width = \"800\"/>\n",
    "\n",
    "Simplifying a bit, in (stochastic) gradient decent algorithm the step size (in weights space) is equal to ***gradient length times learning rate***. Learning rate (LR) is just a number from 0 to 1. In simplest case it's constant - usually chosen to be 0.1. Constant learning rate is default for most algorithms - and it's not the best choice. That's why we introduce learning rate scheduler.\n",
    "\n",
    "***Learning rate scheduler*** is basically a function that modifies LR with each epoch. Usually we want LR to decay with time, but sometimes we also introduce momentum - then LR tends to change quite slowly, which helps to avoid getting stuck in local minima.\n",
    "\n",
    "# Pytorch LR scheduler functions\n",
    "\n",
    "***1. lr_scheduler.LambdaLR*** - sets LR every epoch to the product of initial LR and some function depending from epoch number \n",
    "$$\\alpha_{n+1}=\\alpha_0\\cdot\\lambda(epoch_{n+1})$$\n",
    "\n",
    "***2. lr_scheduler.MultiplicativeLR*** - multiplies LR each epoch by factor equal to function depending from epoch number \n",
    "$$\\alpha_{n+1}=\\alpha_n\\cdot\\lambda(epoch_{n+1})$$\n",
    "\n",
    "***3. lr_scheduler.ExponentialLR*** - decays LR every epoch by $\\gamma$\n",
    "$$\\alpha_{n} = \\alpha_0\\cdot(1-\\gamma)^n$$\n",
    "\n",
    "***4. lr_scheduler.StepLR*** - decays every $n$-th epoch LR by $\\gamma$\n",
    "$$\\alpha_{kn} = \\alpha_0\\cdot(1-\\gamma)^k$$\n",
    "\n",
    "***5. lr_scheduler.MultiStepLR*** - decays LR by $\\gamma$ every \"milestone\", which are a list of integers passed as argument\n",
    "\n",
    "***6. lr_scheduler.ConstantLR*** - decays LR by $\\gamma$ every epoch until $n$-th epoch. Then LR is constant.\n",
    "\n",
    "***7. lr_scheduler.LinearLR*** - changes LR linearly from  $start\\_factor\\cdot\\alpha_0$  to  $end\\_factor\\cdot\\alpha_0$  during first $n$-epochs. Then constant.\n",
    "\n",
    "***8. lr_scheduler.ExponentialLR*** - decays LR by $\\gamma$ every epoch.\n",
    "\n",
    "***9. lr_scheduler.PolynomialLR*** - decays LR every epoch until the $n$-th with some polynomial of given order. I don't truly understand how does that work. \n",
    "\n",
    "***10. lr_scheduler.CosineAnnealingLR*** - \n",
    "\n",
    "<img src = \"annealing.png\" alt = \"an\" width = \"800\"/>\n",
    "\n",
    "***11. lr_scheduler.ChainedScheduler*** - allows you to use two or more regular schedulers in a single step.\n",
    "\n",
    "***12. lr_scheduler.SequentialLR*** - it takes as arguments list of schedulers and a list of \"milestones\". Every milestone the scheduler is changed to the next one in list.\n",
    "\n",
    "***13. lr_scheduler.ReduceLROnPlateau*** - learning rate is constant until we reach the place, where loss function stops decreasing (oscillations smaller than *threshold*). Then learning rate is multiplied by *factor* (default 0.1). If update of LR is smaller than *eps*, the update is ignored.\n",
    "\n",
    "***14. lr_scheduler.CyclicLR*** - the learning rate is changing linearly (with change equal to *step_size*) with each epoch between *base_lr* and *max_lr*. The changes are cyclic.\n",
    "\n",
    "***15. lr_scheduler.OneCycleLR*** - the LR changes linearly from *max_lr* / *div_factor* to *max_lr* nad later from *max_lr* to *max_lr* / *final_div_factor*\n",
    "\n",
    "***16. lr_scheduler.CosineAnnealingWarmRestarts*** -\n",
    "\n",
    "<img src = \"LR2.png\" alt = \"an\" width = \"800\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
